{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Machine Learning with Support Vector Machines and Parameter Tuning\nIn this short micro-project, we'll work on classifying flowers from the famous Iris data set into different categories.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Data\nFisher's Iris data set is a multivariate data set introduced by Sir Ronald Fisher in the 1936 as an example of discriminant analysis.\n\nThe iris dataset contains measurements for 150 iris flowers from three different species.\n\nThe three classes in the Iris dataset:\n\n    Iris-setosa (n=50)\n    Iris-versicolor (n=50)\n    Iris-virginica (n=50)\n\nThe four features of the Iris dataset:\n\n    sepal length in cm\n    sepal width in cm\n    petal length in cm\n    petal width in cm\n\nThe dataset is built into seaborn, so we can use the library to import the data.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "iris = sns.load_dataset('iris')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Exploratory Analysis\nLet's check out the dataset.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "iris.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "sns.pairplot(iris,hue='species')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "A quick look at the pairplot, and we can see that the setosa species seems to be the most separable of the three.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Model Building\nWe'll begin by splitting the data into training and test sets.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nX = iris.drop('species',axis=1)\n\ny = iris['species']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now its time to train a Support Vector Machine Classifier.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.svm import SVC\n\nsv = SVC()\n\nsv.fit(X_train,y_train)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Predictions and Evaluations",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "preds = sv.predict(X_test)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,preds))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(classification_report(y_test,preds))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "And it seems like our model did pretty well!\n\nWe can try and improve the results by tuning the parameters for the classifier. Scikit's inbuilt 'GridSearch' module lets us do that automatically, to an extent. Let's try and use that.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Parameter Tuning using GridSearch",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Defining the initial parameter grid to search in\nparam_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001]}",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "grid = GridSearchCV(SVC(),param_grid,refit=True)\ngrid.fit(X_train,y_train)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### New Predictions and Results",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "grid_predictions = grid.predict(X_test)\n\nprint(confusion_matrix(y_test,grid_predictions))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(classification_report(y_test,grid_predictions))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "A little better this time, with only one point that we weren't able to grab. This might be a good thing in real world applications as we don't want a model that overfits to the taining set completely.\n\nThis concludes our project!",
      "metadata": {}
    }
  ]
}